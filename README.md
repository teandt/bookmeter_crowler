# なにをするツール？
読書メーターの「読んだ本」「積読本」のリストをクロールして取得するツールです。
過去に似たようなツールを作っている人もいましたが古そうだったので。
読んだ日付、本のタイトル、著者名が取得でき、必要に応じて書籍詳細をクロールして本のタイトルの正式版、ページ数、ASINコードあたりを取得してSQLiteで保存します。
出力はそのうち実装するつもりはあるけど、とりあえずはscrapyの出力をjsonで出力するようにしているので最低限それで確認は出来ます。

# 現状
javascript対応無しで取得できるところで必要そうな情報を取得してSQLiteでデータ格納するまでは対応済み。
「読んだ本」のリストをクロールして取得
「積読本」のリストをクロールして取得
取得した「読んだ本」または「積読本」のリストにある本で、書籍詳細が取得されていない場合に取得
は対応済み。
scrapyのSpider確認用にJSONファイル出力するようになっているのはデバッグ用に残しているだけで、今後必要があればDB内のデータから取り出してCSV出力化あたりは対応するかも？
→ ブクログの移行フォーマット対応するかも？

今のところレビュー記録を取得出来るようにするかは悩み中。javascript対応が必要なので必要性があれば？
ブクログ移行のフォーマットにはありそうなのでちょっと気にはなるけどなくても大丈夫じゃない？

# pipでインストールするパッケージ
- scrapy
- python-dotenv
- SQLAlchemy
- scrapy-splash
  
とりあえずpip freezeしておいたので

```
pip install -r requirements
```

# 設定
読書メーターのユーザーIDを指定しておく必要があるので、cd_bookmeter/env/.env にUSER_ID="0000"を設定、自分のユーザIDに変更して実行する。

splash実行のために事前にdocker-compose.ymlを編集して環境を設定しておく。
必要があればポート番号はcr_bookmeter/settings.pyの
```
SPLASH_URL = 'http://localhost:8050'
```
をメンテナンス。

# 実行方法
```
cd cr_bookmeter
docker compose up -d
python3 bookmeter_crawl.py [オプション]
```
オプションはヘルプを見たほうが間違いないと思うけど一応リスト
```
  -st, --stacked        積読本リスト取得
  -rd, --read           読んだ本リスト取得
  -dt, --detail         書籍詳細の取得
  -ckst, --checkstacked
                        DBのデータ確認（積読本）
  -ckrd, --checkread    DBのデータ確認（読んだ本）
  -ckd, --checkdetail   DBのデータ確認（詳細）
```


# 処理の流れのイメージ
bookmeter_crawl.pyで実行。 オプションによってどこをスクレイピングしに行くのかを指定
未読リストor読んだ本リストを取得してSQLiteに保存、更新
書籍詳細オプションが付いている場合、各リストで詳細が取得されていない書籍の書籍ページをクロールして詳細を取得、SQLiteに保存更新

# データ
積読本／読んだ本リストでは以下の情報を取得しておく
```
    num ： 取得順をオートインクリメントで主キーにしておく（どうやら読んだ本リストは同じ本を複数回表示するようなので）
    id : 読書メーターの書籍番号
    short_title ： タイトル（長い場合は後ろが切れる）
    author ： 著者リスト。複数の著者だった場合まとめて入るのが後で課題かも？
    date ：積読本リストでは基本的にNullが入る想定。読んだ本リストだと読書日が入る（不明、で登録すると「日付不明」が入るので要注意）
    url ：読書メーターの書籍詳細Url
```

# ブクログ形式のインポート用CSVファイルの出力
以下のフォーマットで出力します。
```
サービスID, アイテムID, 13桁ISBN, カテゴリ, 評価, 読書状況, レビュー, タグ, 読書メモ(非公開), 登録日時, 読了日
```
出力されるファイルはcsv/booklog.csvとなります。
出力されるファイルの文字コードはS-JISです。UTF-8で出力すると正常に登録されません。

CSVファイル出力する場合、積読本、読んだ本のリストに対応する詳細データが揃っていることが条件で、事前にチェックを行います。
